# -*- coding: utf-8 -*-
"""Proyek Pertama MLTerapan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gsdRgv86AfBLOGIsci3ab_a2gliMyxCb

# Proyek Pertama: Predictive Analytics
Sumber data: [Lung cancer dataset](https://www.kaggle.com/datasets/thedevastator/cancer-patients-and-air-pollution-a-new-link)

## Data Understanding

Deskripsi Dataset  
Berdasarkan informasi dari kaggle, berikut adalah uraian mengenai setiap kolom yang terdapat dalam data:

| Nama Kolom                    | Deskripsi                                                  |
|-------------------------------|------------------------------------------------------------|
| Age                           | Usia pasien. (Numerik)                                    |
| Gender                        | Jenis kelamin pasien. (Kategorikal)                       |
| Air Pollution                 | Tingkat paparan polusi udara pada pasien. (Kategorikal)   |
| Alcohol use                   | Tingkat penggunaan alkohol oleh pasien. (Kategorikal)     |
| Dust Allergy                  | Tingkat alergi debu pada pasien. (Kategorikal)            |
| OccuPational Hazards          | Tingkat bahaya pekerjaan pada pasien. (Kategorikal)       |
| Genetic Risk                  | Tingkat risiko genetik pada pasien. (Kategorikal)         |
| chronic Lung Disease          | Tingkat penyakit paru kronis pada pasien. (Kategorikal)   |
| Balanced Diet                 | Tingkat pola makan seimbang pasien. (Kategorikal)         |
| Obesity                       | Tingkat obesitas pasien. (Kategorikal)                    |
| Smoking                       | Tingkat merokok oleh pasien. (Kategorikal)                |
| Passive Smoker                | Tingkat paparan asap rokok pasif pada pasien. (Kategorikal)|
| Chest Pain                    | Tingkat nyeri dada pada pasien. (Kategorikal)             |
| Coughing of Blood             | Tingkat batuk berdarah pada pasien. (Kategorikal)         |
| Fatigue                       | Tingkat kelelahan pada pasien. (Kategorikal)              |
| Weight Loss                   | Tingkat penurunan berat badan pada pasien. (Kategorikal)  |
| Shortness of Breath           | Tingkat sesak napas pada pasien. (Kategorikal)            |
| Wheezing                      | Tingkat mengi pada pasien. (Kategorikal)                  |
| Swallowing Difficulty         | Tingkat kesulitan menelan pada pasien. (Kategorikal)      |
| Clubbing of Finger Nails      | Tingkat pembengkakan ujung jari pada pasien. (Kategorikal)|

## Data Loading
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

df=pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vT_plEbzzv09pcAAZuPlWM2gGtPDUbUQF7glP4WE7TrEVMKey5AwStl78hk9rXcIfk7Gc7McchtFKg8/pub?output=csv', index_col='index')

""">- Dataset kaggle dipublikasi ulang melalui drive pribadi untuk mempermudah akses
>- Set kolom index agar tidak menjadi features
"""

print(df.head(5))

df.shape

""">Dataset ini berisi data sejumlah 1000 pasien"""

df.head()

""">- Kolom Patient Id bisa dihapus
>- Dataset ini membagi pasien menjadi 3 level, yaitu Low, Medium, dan High
"""

df.drop("Patient Id", axis=1, inplace=True)

df.info()

""">Tipe data sudah sesuai"""

df.describe().T

df.isnull().sum()

""">Tidak ada missing value dalam dataset

## EDA
"""

status_distribution = df['Level'].value_counts()

plt.style.use('ggplot')
plt.pie(status_distribution, labels=status_distribution.index, autopct='%2.1f%%')
plt.title('Status Distribution of Dataset in %')
plt.show()

"""### Label Encoding"""

print('Cancer Level: ', df['Level'].unique())

mapping = {'Low': 0, 'Medium': 1, 'High': 2}
df["Level"].replace(mapping, inplace=True)

print('Cancer Level: ', df['Level'].unique())

""">Encode kolom Level agar menjadi numerik dan dapat divisualisasikan"""

df.head()

"""## Data Visualization"""

fig, ax = plt.subplots(ncols=4, nrows=6, figsize=(20, 20))
ax = ax.flatten()

for i, col in enumerate(df.columns):
    sns.regplot(x=col, y='Level', data=df, lowess=True, ax=ax[i])
    ax[i].set_title(col.title())

plt.tight_layout(pad=0.1, w_pad=0.6, h_pad=1)
plt.show()

fig, ax = plt.subplots(ncols=4, nrows=6, figsize=(20, 20))
ax = ax.flatten()

columns_to_plot = [col for col in df.columns if col != 'Level']

for i, col in enumerate(columns_to_plot):
    sns.violinplot(x=df['Level'],
                   y=df[col], data=df, hue_order=df['Level'].unique(), palette='Reds', ax=ax[i])
    ax[i].set_title(col.title())

plt.tight_layout(pad=0.1, w_pad=0.2, h_pad=2.5)
plt.show()

from scipy.stats import norm

fig, ax = plt.subplots(ncols=8, nrows=3, figsize=(24, 12))
ax = ax.flatten()

for i, (column_name, data) in enumerate(df.items()):
    mu, sigma = norm.fit(data)

    sns.histplot(data,
                 kde=True,
                 bins=20,
                 ax=ax[i],
                 label=f'$\mu={mu:.1f}$\n$\sigma={sigma:.1f}$')

    ax[i].set_title(column_name.title())
    ax[i].legend()

plt.tight_layout(pad=0.2, w_pad=0.2, h_pad=1.0)
plt.show()

correlation_matrix = df.corr()

plt.figure(figsize=(20,15))
sns.heatmap(df.corr(), annot=True, cmap='Reds')
plt.title('Correlation Matrix of All Features', size=20)
plt.show()

"""## Data Preparation"""

X=df.drop('Level',axis=1)
y=df['Level']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Modeling"""

def models(X_train, y_train):

    y_train = y_train.values.ravel()

    lr = LogisticRegression()
    lr.fit(X_train, y_train)

    dt = DecisionTreeClassifier()
    dt.fit(X_train,y_train)

    rf = RandomForestClassifier()
    rf.fit(X_train, y_train)

    gb = GradientBoostingClassifier()
    gb.fit(X_train, y_train)

    knn = KNeighborsClassifier()
    knn.fit(X_train, y_train)

    return lr, dt, rf, gb, knn

y_test = y_test.values.ravel()

lr, dt, rf, gb, knn = models(X_train, y_train)

y_pred_lr = lr.predict(X_test)
y_pred_dt = dt.predict(X_test)
y_pred_rf = rf.predict(X_test)
y_pred_gb = gb.predict(X_test)
y_pred_knn = knn.predict(X_test)

"""## Evaluation"""

def evaluate_models(X_test, y_test, models):
    results = []
    for name, model in models.items():
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='macro')
        recall = recall_score(y_test, y_pred, average='macro')

        results.append({'Model': name, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall})

    return pd.DataFrame(results)

models = {'Logistic Regression': lr, 'Decision Tree': dt, 'Random Forest': rf, 'Gradient Boosting': gb, 'KNN': knn}

results_df = evaluate_models(X_test, y_test, models)

print(results_df)

"""## Hyperparameter Tuning"""

folds = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)

def grid_search(model, folds, params, scoring):
    grid_search = GridSearchCV(model,
                               cv=folds,
                               param_grid=params,
                               scoring=scoring,
                               n_jobs=1,
                               verbose=1)
    return grid_search

def print_best_score_params(model):
    print('Best Score: ', model.best_score_)
    print('Best Hyperparameters: ', model.best_params_)

"""### LR"""

lr = LogisticRegression()
lr_params = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],
             'solver': ['liblinear', 'saga'],
             'C': [0.1, 1, 10],
             'l1_ratio': [0.5]}
grid_search_lr = grid_search(lr, folds, lr_params, scoring=None)

grid_search_lr.fit(X_train, y_train)

print_best_score_params(grid_search_lr)

lr = LogisticRegression(C = 10, l1_ratio = 0.5, penalty='l1', solver='liblinear')
lr.fit(X_train, y_train)

lr_y_pred = lr.predict(X_test)

print('Accuracy: ', accuracy_score(y_test, lr_y_pred))

cm = confusion_matrix(y_test, lr_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, lr_y_pred))

"""### DT"""

dt = DecisionTreeClassifier(random_state=42)
dt_params = {
    'max_depth': [5, 10, 20, 30],
    'min_samples_leaf': [5, 10, 20, 30],
}
grid_search_dt = grid_search(dt, folds, dt_params, scoring='roc_auc_ovr')

grid_search_dt.fit(X_train, y_train)

print_best_score_params(grid_search_dt)

dt = DecisionTreeClassifier(max_depth= 5, min_samples_leaf=5)
dt.fit(X_train, y_train)

dt_y_pred = dt.predict(X_test)

print('Accuracy: ', accuracy_score(y_test, dt_y_pred))

cm = confusion_matrix(y_test, dt_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, dt_y_pred))

"""### RF"""

rf = RandomForestClassifier(random_state=123)

rf_params = {
    'n_estimators': [200, 500],
    'max_features': ['sqrt', 'log2'],
    'max_depth' : [15 ,20],
    'criterion' :['gini', 'entropy']
}
grid_search_rf = grid_search(rf, folds, rf_params, scoring='roc_auc_ovr')

y_train = y_train.values.ravel()

grid_search_rf.fit(X_train, y_train)

print_best_score_params(grid_search_rf)

rf = RandomForestClassifier(
    criterion='gini',
    max_depth=15,
    max_features='sqrt',
    n_estimators=200
)
rf.fit(X_train, y_train)

rf_y_pred = rf.predict(X_test)

print('Accuracy: ', accuracy_score(y_test, rf_y_pred))

cm = confusion_matrix(y_test, rf_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, rf_y_pred))

"""### GB"""

gb = GradientBoostingClassifier(random_state=123)

gb_params = {
    'max_depth': [5, 8],
    'n_estimators': [200, 300],
    'learning_rate': [0.01, 0.1],
    'max_features': ['auto', 'sqrt', 'log2']
}
grid_search_gb = grid_search(gb, folds, gb_params, scoring='accuracy')

grid_search_gb.fit(X_train, y_train)

print_best_score_params(grid_search_gb)

gb = GradientBoostingClassifier(
    learning_rate=0.01,
    max_depth=5,
    max_features='sqrt',
    n_estimators=200
)
gb.fit(X_train, y_train)

gb_y_pred = gb.predict(X_test)

print('Accuracy: ', accuracy_score(y_test, gb_y_pred))

cm = confusion_matrix(y_test, gb_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, gb_y_pred))

"""### KNN"""

knn = KNeighborsClassifier(n_neighbors=8, weights='distance')
knn_params = {'n_neighbors' : [1,2,3,4,8,9,14,19,20,25],
              'weights' : ['uniform','distance'],
              'algorithm' : ['auto','ball_tree','kd_tree','brute']
}
grid_search_knn = GridSearchCV(knn, knn_params, cv = 10, scoring="accuracy")

grid_search_knn.fit(X_train, y_train)

print("Best score:",grid_search_knn.best_score_)
print("Best parameters:",grid_search_knn.best_estimator_)

knn = KNeighborsClassifier(
    n_neighbors=19,
    weights='distance',
)
knn.fit(X_train, y_train)

knn_y_pred = knn.predict(X_test)

print('Accuracy: ', accuracy_score(y_test, knn_y_pred))

cm = confusion_matrix(y_test, knn_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'HIgh'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, knn_y_pred))